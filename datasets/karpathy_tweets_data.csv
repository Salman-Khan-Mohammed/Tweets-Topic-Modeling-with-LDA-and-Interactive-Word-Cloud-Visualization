link,text,username,user,likes,quotes,retweets,comments
https://twitter.com/karpathy/status/1617979122625712128#m,The hottest new programming language is English,karpathy,Andrej Karpathy,33812,738,4679,863
https://twitter.com/karpathy/status/1801311713842893161#m,"New simulation hypothesis drop. Maybe the simulation is not physical and exact but neural and approximate. i.e. not about simulating fields or particles with physical equations but a giant Diffusion Transformer++ creating a large ""dream"".",karpathy,Andrej Karpathy,4718,142,344,488
https://twitter.com/karpathy/status/1801305852735115357#m,"wow. The new model from @LumaLabsAI extending images into videos is really something else. I understood intuitively that this would become possible very soon, but it's still something else to see it and think through future iterations of.  A few more examples around, e.g. the‚Ä¶",karpathy,Andrej Karpathy,5882,54,598,122
https://twitter.com/karpathy/status/1800242310116262150#m,"Actually, really liked the Apple Intelligence announcement. It must be a very exciting time at Apple as they layer AI on top of the entire OS. A few of the major themes.  Step 1 Multimodal I/O. Enable text/audio/image/video capability, both read and write. These are the native‚Ä¶",karpathy,Andrej Karpathy,9724,187,1194,332
https://twitter.com/karpathy/status/1800223553989886447#m,"If you tuned in to WWDC to see what Apple is doing with AI, we're all probably thinking the same thing around now 50 minutes into it... ü´†",karpathy,Andrej Karpathy,5361,81,243,297
https://twitter.com/karpathy/status/1799949853289804266#m,"üìΩÔ∏è New 4 hour (lol) video lecture on YouTube: ""Let‚Äôs reproduce GPT-2 (124M)"" https://tube.habedieeh.re/l8pRSuU81PU  The video ended up so long because it is... comprehensive: we start with empty file and end up with a GPT-2 (124M) model: - first we build the GPT-2 network  - then we optimize‚Ä¶",karpathy,Andrej Karpathy,15979,407,2324,422
https://twitter.com/karpathy/status/1797317096155852946#m,"Example here is the llm.c GPT-3 (124M) training on FineWeb (figure cropped at 250B tokens), we seem to surpass GPT-3 HellaSwag (green line) at ~150B tokens, per paper expected this to be at 300B tokens. Will re-run with FineWeb-Edu.    I do want to be a bit careful on conclusions‚Ä¶",karpathy,Andrej Karpathy,402,1,17,8
https://twitter.com/karpathy/status/1797314805772300661#m,"In llm.c pretraining we were already mildly perplexed why seem to be outperforming GPT-2 & 3 (124M) training on just 10B tokens instead of something closer to 100-300B, per the original papers. I suspect a good chunk of it may be just the dataset quality, so I'm eager to retrain‚Ä¶",karpathy,Andrej Karpathy,610,4,24,17
https://twitter.com/karpathy/status/1797313173449764933#m,"Awesome and highly useful: FineWeb-Edu üìöüëè High quality LLM dataset filtering the original 15 trillion FineWeb tokens to 1.3 trillion of the highest (educational) quality, as judged by a Llama 3 70B. +A highly detailed paper.  Turns out that LLMs learn a lot better and faster‚Ä¶",karpathy,Andrej Karpathy,3606,52,509,59
https://twitter.com/karpathy/status/1796305221813198946#m,Can I just say I loooove Suno. Some of my favorites:  Dog dog dog dog dog dog dog dog woof woof https://suno.com/song/1783c864-18fb-440f-bc51-15701a19e4b5 Chemical elements https://suno.com/song/5f324463-08a7-490e-b9c5-f8e2d399baa9 train_gpt2.c header (who did this lol) https://suno.com/song/2a210337-62fc-49f8-8850-9af12e06e6e0 Suno tutorial (in Suno!): https://suno.com/song/d960e84a-1b03-46a2-999e-2a896a56bd57  Many‚Ä¶,karpathy,Andrej Karpathy,2373,36,222,178
https://twitter.com/karpathy/status/1795980744436932871#m,Apparently today is the 4th year anniversary of GPT-3! https://arxiv.org/abs/2005.14165  Which I am accidentally celebrating by re-training the smallest model in the miniseries right now :). HellaSwag 33.7 (Appendix H) almost reached this a few steps ago (though this is only 45% of the‚Ä¶,karpathy,Andrej Karpathy,2503,23,263,58
https://twitter.com/karpathy/status/1795873666481402010#m,"Nice, a serious contender to @lmsysorg in evaluating LLMs has entered the chat.  LLM evals are improving, but not so long ago their state was very bleak, with qualitative experience very often disagreeing with quantitative rankings.  This is because good evals are very difficult‚Ä¶",karpathy,Andrej Karpathy,2450,23,324,45
https://twitter.com/karpathy/status/1795484547267834137#m,"# Reproduce GPT-2 (124M) in llm.c in 90 minutes for $20 ‚ú®  The GPT-2 (124M) is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. For example, with llm.c you can now reproduce this model on one 8X‚Ä¶",karpathy,Andrej Karpathy,5250,97,698,160
https://twitter.com/RuiHuang_art/status/1793758847292854314#m,Welcome home,karpathy,Rui Huang,14850,104,1861,158
https://twitter.com/naklecha/status/1792244347225641338#m,"today, im excited to release a repository that implements llama3 from scratch -- every matrix multiplication from attention across multiple heads, positional encoding and every other layer in between has been carefully unwrapped & explained. have fun :)  https://github.com/naklecha/llama3-from-scratch",karpathy,naklecha,5130,87,663,136
https://twitter.com/karpathy/status/1790373216537502106#m,The killer app of LLMs is Scarlett Johansson. You all thought it was math or something,karpathy,Andrej Karpathy,11831,230,1060,331
https://twitter.com/karpathy/status/1790092394571898903#m,üòä,karpathy,Andrej Karpathy,3897,22,190,160
https://twitter.com/karpathy/status/1790076925508977096#m,"They are releasing a combined text-audio-vision model that processes all three modalities in one single neural network, which can then do real-time voice translation as a special case afterthought, if you ask it to.  (fixed it for you)",karpathy,Andrej Karpathy,8244,104,784,213
https://twitter.com/karpathy/status/1789605356617752724#m,"Anyone else find themselves estimating the ""GPT grade"" of things you hear/read? When something is poorly written or generic, it's ""GPT-2 grade"" content. When something is lit, you can complement it as being ""GPT-7 grade"" etc.  This reminds me of a fun side project I had saved for‚Ä¶",karpathy,Andrej Karpathy,1334,18,105,70
https://twitter.com/karpathy/status/1789590397749957117#m,"Nice new read on tokenization! You've heard about the SolidGoldMagikarp token, which breaks GPT-2 because it was present in the training set of the Tokenizer, but not the LLM later.  This paper digs in in a lot more depth and detail, on a lot more models, discovering a less‚Ä¶",karpathy,Andrej Karpathy,2896,22,391,48
https://twitter.com/karpathy/status/1786537319576789425#m,"# CUDA/C++ origins of Deep Learning  Fun fact many people might have heard about the ImageNet / AlexNet moment of 2012, and the deep learning revolution it started. https://en.wikipedia.org/wiki/AlexNet  What's maybe a bit less known is that the code backing this winning submission to the‚Ä¶",karpathy,Andrej Karpathy,7206,97,948,171
https://twitter.com/karpathy/status/1786461447654125625#m,"Day 24 of llm.c: we now do multi-GPU training, in bfloat16, with flash attention, directly in ~3000 lines of C/CUDA, and it is FAST! üöÄ  We're running ~7% faster than PyTorch nightly, with no asterisks, i.e. this baseline includes all modern & standard bells-and-whistles: mixed‚Ä¶",karpathy,Andrej Karpathy,6842,110,707,214
https://twitter.com/karpathy/status/1786138081978171656#m,The living portraits at Hogwarts are now technologically quite possible. Would like to buy one and enter my house this way,karpathy,Andrej Karpathy,2691,32,172,147
https://twitter.com/karpathy/status/1786085254006202541#m,"Clearly LLMs must one day run in Space  Step 1 we harden llm.c to pass the NASA code standards and style guides, certifying that the code is super safe, safe enough to run in Space. https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code (see the linked PDF) LLM training/inference in principle should be super‚Ä¶",karpathy,Andrej Karpathy,4854,116,509,327
https://twitter.com/hughbzhang/status/1785877026794356858#m,"Data contamination is a huge problem for LLM evals right now. At Scale, we created a new test set for GSM8k *from scratch* to measure overfitting and found evidence that some models (most notably Mistral and Phi) do substantially worse on this new test set compared to GSM8k.",karpathy,Hugh Zhang,1083,52,240,36
https://twitter.com/jeremyphoward/status/1784717268368367665#m,"There's a new bill, SB-1047 ""Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"".  I think it could do a great deal of harm to startups, American innovation, open source, and safety. So I've written a response to the authors: üßµ https://www.answer.ai/posts/2024-04-29-sb1047.html",karpathy,Jeremy Howard,1236,31,338,36
https://twitter.com/karpathy/status/1782871281849032977#m,Money can't buy happiness. Just like an H100. H100 = happiness.,karpathy,Andrej Karpathy,5004,53,311,199
https://twitter.com/karpathy/status/1781387674978533427#m,"üî•llm.c update: Our single file of 2,000 ~clean lines of C/CUDA code now trains GPT-2 (124M) on GPU at speeds ~matching PyTorch (fp32, no flash attention) https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu  On my A100 I'm seeing 78ms/iter for llm.c and 80ms/iter for PyTorch. Keeping in mind this is fp32,‚Ä¶",karpathy,Andrej Karpathy,5358,68,582,156
https://twitter.com/karpathy/status/1781047292486914189#m,"The model card has some more interesting info too: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md  Note that Llama 3 8B is actually somewhere in the territory of Llama 2 70B, depending on where you look. This might seem confusing at first but note that the former was trained for 15T tokens, while the‚Ä¶",karpathy,Andrej Karpathy,1179,18,109,32
https://twitter.com/karpathy/status/1781028605709234613#m,"Congrats to @AIatMeta on Llama 3 release!! üéâ https://ai.meta.com/blog/meta-llama-3/ Notes:  Releasing 8B and 70B (both base and finetuned) models, strong-performing in their model class (but we'll see when the rankings come in @ @lmsysorg  :)) 400B is still training, but already encroaching‚Ä¶",karpathy,Andrej Karpathy,7959,143,1050,144
https://twitter.com/karpathy/status/1780730292837507092#m,Consider being a labeler for an LLM. The prompt is ‚Äúgive me a random number between 1 and 10‚Äù. What SFT & RM labels do you contribute? What does this do the network when trained on?  In subtle way this problem is present in every prompt that does not have a single unique answer.,karpathy,Andrej Karpathy,1312,13,87,138
https://twitter.com/karpathy/status/1780692023970038259#m,"The history of computing is repeating in an echo, except replace computers that do precise arithmetic on bytes with computers that do statistical arithmetic on tokens.",karpathy,Andrej Karpathy,2641,27,298,83
https://twitter.com/karpathy/status/1780684098773876941#m,"# scheduling workloads to run on humans  Some computational workloads in human organizations are best ""run on a CPU"": take one single, highly competent person and assign them a task to complete in a single-threaded fashion, without synchronization. Usually the best fit when‚Ä¶",karpathy,Andrej Karpathy,3033,48,379,89
https://twitter.com/karpathy/status/1780673514569396552#m,üß†: ‚ÄúLet‚Äôs but this (text)book! Nice and now‚Ä¶  instead of reading it‚Ä¶ let‚Äôs buy another one!‚Äù üí°  All of the dopamine is generated only at the point of resolving to read something. After that there is no juice left üòÖ,karpathy,Andrej Karpathy,3272,46,172,193
https://twitter.com/karpathy/status/1779354343013269929#m,"THE REVENGE OF PYTORCH just kidding :)  @cHHillee (from PyTorch team) was kindly able to help improve the PyTorch baseline, done by 1) upgrading to nightly, 2) using the ""compound"" F.sdpa (scaled dot product attention) layer directly, and turning on a torch compile flag:‚Ä¶",karpathy,Andrej Karpathy,1295,7,46,33
https://twitter.com/karpathy/status/1779272336186978707#m,"Highly amusing update, ~18 hours later:  llm.c is now down to 26.2ms/iteration, exactly matching PyTorch (tf32 forward pass). We discovered a bug where we incorrectly called cuBLAS in fp32 mathmode ü§¶‚Äç‚ôÇÔ∏è. And ademeure contributed a more optimized softmax kernel for very long rows‚Ä¶",karpathy,Andrej Karpathy,6337,122,575,167
https://twitter.com/karpathy/status/1778988957713477778#m,"A few new CUDA hacker friends joined the effort and now llm.c is only 2X slower than PyTorch (fp32, forward pass) compared to 4 days ago, when it was at 4.2X slower üìà  The biggest improvements were: - turn on TF32 (NVIDIA TensorFLoat-32) instead of FP32 for matmuls. This is a‚Ä¶",karpathy,Andrej Karpathy,4431,28,377,117
https://twitter.com/karpathy/status/1778876244014354655#m,"torch.compile is cool but  LLM compile: takes your .py repo as string and outputs a brand new, custom, from scratch, minimal code repository directly running your network in highly optimized CUDA",karpathy,Andrej Karpathy,2111,10,114,58
https://twitter.com/karpathy/status/1778841713605525889#m,"This post became popular; Few more thoughts / pointers on the topic for the interested reader.  Example of the complexity involved: @cHHillee has a great post ""Making Deep Learning Go Brrrr From First Principles"" https://horace.io/brrr_intro.html I was always struck by this diagram from‚Ä¶",karpathy,Andrej Karpathy,795,2,56,22
https://twitter.com/karpathy/status/1778153659106533806#m,"# explaining llm.c in layman terms  Training Large Language Models (LLMs), like ChatGPT, involves a large amount of code and complexity.  For example, a typical LLM training project might use the PyTorch deep learning library. PyTorch is quite complex because it implements a very‚Ä¶",karpathy,Andrej Karpathy,10057,228,1283,422
https://twitter.com/karpathy/status/1778128793166856368#m,"Okay I did a first quick pass of naive CUDA kernels for the forward pass of GPT-2 and pushed everything to one file in llm.c, Still only ~1000 lines of code: https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu  Current per iteration timings on my Lambda box <3 A100 40GB PCIe, B=4, T=1024: - llm.c: 111ms -‚Ä¶",karpathy,Andrej Karpathy,3882,31,336,80
https://twitter.com/karpathy/status/1777493157485437009#m,"Btw writing the llm.c training code would imo be a very interesting, impressive, self-contained and very meta challenge for LLM agents. The prompt is:  Take the PyTorch code train_gpt2.py And write, compile and unit test a single .c file that reproduces the training: train_gpt2.c‚Ä¶",karpathy,Andrej Karpathy,3023,17,170,77
https://twitter.com/karpathy/status/1777481372636246491#m,"I added a quick crappy tutorial on how PyTorch layers are moved to C, with a few possibly helpful pointers: https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md",karpathy,Andrej Karpathy,2678,11,248,47
https://twitter.com/karpathy/status/1777427952881541524#m,"Once you have the forward/backward, the rest of it (data loader, Adam update, etc) are mostly trivial.  The real fun starts now though: I am now porting this to CUDA layer by layer so that it can be made efficient, perhaps even coming within reasonable fraction of PyTorch, but‚Ä¶",karpathy,Andrej Karpathy,1087,11,40,53
https://twitter.com/karpathy/status/1777427950021026006#m,"Once you have all the layers, you just string all it all together. Not gonna lie, this was quite tedious and masochistic to write because you have to make sure all the pointers and tensor offsets are correctly arranged.  Left: we allocate a single 1D array of memory and then‚Ä¶",karpathy,Andrej Karpathy,735,4,22,10
https://twitter.com/karpathy/status/1777427947126936026#m,"You can look at the raw training implementation here: https://github.com/karpathy/llm.c/blob/master/train_gpt2.c  You'll see that we allocate all the required memory a single time in the beginning in one large block of 1D memory. From there on during training, no memory gets created or destroyed, so we stay at‚Ä¶",karpathy,Andrej Karpathy,813,3,33,13
https://twitter.com/karpathy/status/1777427944971083809#m,"Have you ever wanted to train LLMs in pure C without 245MB of PyTorch and 107MB of cPython? No? Well now you can! With llm.c: https://github.com/karpathy/llm.c  To start, implements GPT-2 training on CPU/fp32 in only ~1,000 lines of clean code. It compiles and runs instantly, and exactly‚Ä¶",karpathy,Andrej Karpathy,13271,328,1927,307
https://twitter.com/karpathy/status/1776269310631235806#m,"Returning from an experimental ~2 week detox from the internet. Main takeaway is that I didn't realize how unsettled the mind can get when over-stimulating on problems/information (like a stirred liquid), and ~2 weeks is enough to settle into a lot more zen state.  I'm struck by‚Ä¶",karpathy,Andrej Karpathy,12535,162,930,534
https://twitter.com/karpathy/status/1773117863231914337#m,"Thank you @stephzhan for the chat and @sequoia for hosting, pleasure to come by!",karpathy,Andrej Karpathy,1865,19,174,63
https://twitter.com/karpathy/status/1770164518758633590#m,"Follow along the @__tinygrad__  saga, who are (very publicly!) trying to build your commodity ~petaflop compute node.  tinybox specs: https://tinygrad.org the youtube videos form @realGeorgeHotz are actually quite great and entertaining, featuring the signature blend of‚Ä¶",karpathy,Andrej Karpathy,3365,10,239,93
https://twitter.com/karpathy/status/1767616494752731633#m,"+1 to the best AI newsletter atm that I enjoy skimming, great/ambitious work by @swyx & friends:  https://buttondown.email/ainews/archive/  ""Skimming"" because they are very long. Not sure how it is built, sounds like there is a lot of LLM aid going on indexing ~356 Twitters, ~21 Discords, etc.",karpathy,Andrej Karpathy,2041,15,219,81
https://twitter.com/karpathy/status/1767598414945292695#m,"# automating software engineering  In my mind, automating software engineering will look similar to automating driving. E.g. in self-driving the progression of increasing autonomy and higher abstraction looks something like:  1. first the human performs all driving actions‚Ä¶",karpathy,Andrej Karpathy,11285,281,1913,381
https://twitter.com/karpathy/status/1766541375842009185#m,(btw ‚Äúuntrusted‚Äù and ‚Äúattacker-controlled‚Äù are technical terms in computer security),karpathy,Andrej Karpathy,761,7,20,32
https://twitter.com/karpathy/status/1766509149297189274#m,"Reading a tweet is a bit like downloading an (attacker-controlled) executable that you instantly run on your brain. Each one elicits emotions, suggests knowledge, nudges world-view.  In the future it might feel surprising that we allowed direct, untrusted information to brain.",karpathy,Andrej Karpathy,10663,528,1462,789
https://twitter.com/karpathy/status/1765473722985771335#m,"Beautiful work / attention to detail trying to get Gemma to finetune correctly. There are so many foot guns here to be super careful with. All of these issues don't throw any errors, they silently make your network worse.  A great example of what I wrote about in my ""A Recipe for‚Ä¶",karpathy,Andrej Karpathy,2702,18,316,88
https://twitter.com/karpathy/status/1765424847705047247#m,"Nice read on the rarely-discussed-in-the-open difficulties of training LLMs. Mature companies have dedicated teams maintaining the clusters. At scale, clusters leave the realm of engineering and become a lot more biological, hence e.g. teams dedicated to ""hardware health"".  It‚Ä¶",karpathy,Andrej Karpathy,4280,63,519,110
https://twitter.com/karpathy/status/1764731169109872952#m,"Claude 3 takes on the Tokenization book chapter challenge :) context: https://bird.habedieeh.re/karpathy/status/1760740503614836917  Definitely looks quite nice, stylistically!   If you look closer there are a number of subtle issues / hallucinations. One example there is a claim that ""hello world"" tokenizes into 3‚Ä¶",karpathy,Andrej Karpathy,3955,46,440,123
https://twitter.com/karpathy/status/1762621031121145996#m,"Setting up my shiny new fully maxed out Space Black MacBook Pro M3 Max 128GB 16-inch (upgrading from an M1 Air). I always like to set up the new one with a clean slate, from scratch - this time I will not allow my dev configuration to get out of hand. Then we'll talk to it.",karpathy,Andrej Karpathy,5882,49,144,372
https://twitter.com/karpathy/status/1761467904737067456#m,"Love letter to @obsdmd to which I very happily switched to for my personal notes. My primary interest in Obsidian is not even for note taking specifically, it is that Obsidian is around the state of the art of a philosophy of software and what it could be.  - Your notes are‚Ä¶",karpathy,Andrej Karpathy,9121,247,899,385
https://twitter.com/karpathy/status/1760807877424640386#m,"Ok I wrote the following example of what I am imagining:  https://github.com/karpathy/minbpe/blob/master/lecture.md  This is me doing this task manually, of watching the video and translating it to a markdown post. I only made it to the ~4min mark in the video (i.e. 3% done) and this already took about 30 minutes‚Ä¶",karpathy,Andrej Karpathy,825,4,42,39
https://twitter.com/karpathy/status/1760740503614836917#m,Fun LLM challenge that I'm thinking about: take my 2h13m tokenizer video and translate the video into the format of a book chapter (or a blog post) on tokenization. Something like:  1. Whisper the video 2. Chop up into segments of aligned images and text 3. Prompt engineer an LLM‚Ä¶,karpathy,Andrej Karpathy,4899,57,372,213
https://twitter.com/karpathy/status/1760388761349927356#m,"# on technical accessibility  One interesting observation I think back to often: - when I first published the micrograd repo, it got some traction on GitHub but then somewhat stagnated and it didn't seem that people cared much. - then I made the video building it from scratch,‚Ä¶",karpathy,Andrej Karpathy,7005,167,777,332
https://twitter.com/karpathy/status/1760350892317098371#m,"Seeing as I published my Tokenizer video yesterday, I thought it could be fun to take a deepdive into the Gemma tokenizer.   First, the Gemma technical report [pdf]:  https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf  says: ""We use a subset of the SentencePiece tokenizer (Kudo and Richardson, 2018) of‚Ä¶",karpathy,Andrej Karpathy,4605,43,473,183
https://twitter.com/karpathy/status/1760022429605474550#m,"""My benchmark for large language models"" https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html  Nice post but even more than the 100 tests specifically, the Github code looks excellent - full-featured test evaluation framework, easy to extend with further tests and run against many LLMs.‚Ä¶",karpathy,Andrej Karpathy,3926,26,452,175
https://twitter.com/karpathy/status/1759996554747027865#m,"The actual link to the lecture: https://tube.habedieeh.re/watch?v=zduSFxRajkE  (at the end of the thread here (sorry) otherwise X really really dislikes external links and would bury this post. I could eventually upload here too, for now X is missing a lot of very nice features, chapters especially)",karpathy,Andrej Karpathy,1318,18,107,33
https://twitter.com/karpathy/status/1759996553425760524#m,"Also, releasing new repository on GitHub: minbpe Minimal, clean, code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. https://github.com/karpathy/minbpe  In the video we essentially build minbpe from scratch. Don't miss the exercise.md to build your‚Ä¶",karpathy,Andrej Karpathy,1113,7,67,19
https://twitter.com/karpathy/status/1759996551378940395#m,"We will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.",karpathy,Andrej Karpathy,2589,86,280,59
https://twitter.com/karpathy/status/1759996549109776702#m,"New (2h13m üòÖ) lecture: ""Let's build the GPT Tokenizer""  Tokenizers are a completely separate stage of the LLM pipeline: they have their own training set, training algorithm (Byte Pair Encoding), and after training implement two functions: encode() from strings to tokens, and‚Ä¶",karpathy,Andrej Karpathy,14222,346,1955,381
https://twitter.com/karpathy/status/1757986972512239665#m,My calendar this week,karpathy,Andrej Karpathy,12296,230,321,725
https://twitter.com/karpathy/status/1757600075281547344#m,"Hi everyone yes, I left OpenAI yesterday. First of all nothing ""happened"" and it‚Äôs not a result of any particular event, issue or drama (but please keep the conspiracy theories coming as they are highly entertaining :)). Actually, being at OpenAI over the last ~year has been‚Ä¶",karpathy,Andrej Karpathy,22763,556,1442,1576
https://twitter.com/karpathy/status/1757080501712830828#m,"Do people have opinions for the easiest way to host a static website today? Not just the hosting but custom domain, ssl, deploy with git push",karpathy,Andrej Karpathy,847,18,19,330
https://twitter.com/karpathy/status/1757075417775964290#m,"The internet used to be ‚ú® fun‚ú® https://projects.kwon.nyc/internet-is-fun/  I remember visiting my friend‚Äôs websites. They were ugly and quirky and it was awesome. You wondered who‚Äôd stop by yours. They were a labor of love and a medium of self-expression, not your LinkedIn.  We can fight this.",karpathy,Andrej Karpathy,3598,52,261,151
https://twitter.com/karpathy/status/1756380066580455557#m,"# on shortification of ""learning""  There are a lot of videos on YouTube/TikTok etc. that give the appearance of education, but if you look closely they are really just entertainment. This is very convenient for everyone involved : the people watching enjoy thinking they are‚Ä¶",karpathy,Andrej Karpathy,14212,554,2668,675
https://twitter.com/karpathy/status/1754019554697855449#m,"[~2 more hours later]  Okay I upgraded to the (latest) 1.0.2. and some of the jank got a bit better, e.g. my Disney+ app now starts ok, and I was able to watch some movies in a cool 3D theater. I am a bit salty that the app asks you to enter your password twice (second time to‚Ä¶",karpathy,Andrej Karpathy,758,4,27,39
https://twitter.com/karpathy/status/1753842145075818707#m,"Early thoughts on the Apple Vision Pro (I ended up buying directly in store last evening). I'm about 3 hours in, between late last night and this morning.  The first major thing that must be said is WOW - the visual clarity is way beyond anything that came before. But, a bit‚Ä¶",karpathy,Andrej Karpathy,5988,118,437,252
https://twitter.com/karpathy/status/1753533021192630602#m,I didn't realize you'd be able to just walk into an Apple Store and buy one today. I played myself.,karpathy,Andrej Karpathy,848,3,6,42
https://twitter.com/karpathy/status/1753500976412254481#m,"Not me jealously looking at all the people getting their Apple Vision Pro today...  I woke up to order mine a few days ago at 5am too, but I selected mail delivery instead of pickup, and it only tells you after you order and pay that this moves your time from Feb 2 -> Feb 6. And‚Ä¶",karpathy,Andrej Karpathy,3795,27,149,146
https://twitter.com/natfriedman/status/1752831181677305952#m,"Applications are open for batch 3 of aigrant.com for pre-seed and seed-stage companies building AI products! Deadline is Feb 16.  As an experiment, this batch we are offering the option of either receiving $250k on an uncapped note, or $2.5M at a $25M cap.",karpathy,Nat Friedman,1264,30,162,59
https://twitter.com/karpathy/status/1751350002281300461#m,"Thinking about the ideal blogging platform:  1. Writing:  - in markdown - with full WYSIWYG, not just split view (think: Typora) - super easy to copy paste and add images 2. Deploying: - renders into static pages (think: Jekyll) - super simple, super minimal html with no bloat -‚Ä¶",karpathy,Andrej Karpathy,4038,71,277,479
https://twitter.com/karpathy/status/1748788330563867032#m,"Stop, this has nothing to do with neuralink haha. Anway that's only Stage 1 of enlightenment. Stage 2 of enlightenment is that the ideal training data for an LLM is not training data at all. It's the thumbs up you get from someone who reads it. But you make do with what there is.",karpathy,Andrej Karpathy,685,6,27,47
https://twitter.com/karpathy/status/1748784260318990496#m,The ideal training data for an LLM is not what you wrote. It's the full sequence of your internal thoughts and all the individual edits while you wrote it. But you make do with what there is.,karpathy,Andrej Karpathy,3445,88,274,188
https://twitter.com/karpathy/status/1748043513156272416#m,"Prompt engineering (or rather ""Flow engineering"") intensifies for code generation. Great reading and a reminder of how much alpha there is (pass@5 19% to 44%) in moving from a naive prompt:answer paradigm to a ""flow"" paradigm, where the answer is constructed iteratively.",karpathy,Andrej Karpathy,3335,69,552,126
https://twitter.com/tldraw/status/1747625230036529643#m,what if you could drag around any image like this?,karpathy,tldraw,1909,35,166,46
https://twitter.com/amasad/status/1747666962749284468#m,"The open-source AI revolution hasn‚Äôt happened yet!  Yes we have impressive open-weights models, and thank you to those publishing weights, but if you can‚Äôt reproduce the model then it‚Äôs not truly open-source.  Imagine if Linux published only a binary without the codebase. Or‚Ä¶",karpathy,Amjad Masad,1821,43,254,96
https://twitter.com/karpathy/status/1746946080628195770#m,"# Portrayals of AI People sometimes read a bit too specifically into my bio ""Building a kind of JARVIS"".   I name JARVIS in general terms only, as one of my favorite popular portrayals of an AI - a helpful, conversational, empowering e/ia automation. An aid against evil and‚Ä¶",karpathy,Andrej Karpathy,1960,36,161,319
https://twitter.com/karpathy/status/1746609206889951642#m,"Idea: safeLinux. All the same programs you know and love but now upgraded with safety to stop bad actors right in their tracks.  $ ls I'm sorry, I cannot list the files in this directory because one or more files may contain unsafe content. Can I help you with anything else?  üòÖ",karpathy,Andrej Karpathy,2271,29,150,200
https://twitter.com/karpathy/status/1745921205020799433#m,"I touched on the idea of sleeper agent LLMs at the end of my recent video, as a likely major security challenge for LLMs (perhaps more devious than prompt injection).  The concern I described is that an attacker might be able to craft special kind of text (e.g. with a trigger‚Ä¶",karpathy,Andrej Karpathy,5069,109,723,222
https://twitter.com/karpathy/status/1744200417784045799#m,"(I‚Äôll add thoughts to thread as they come up. ) Thinking of these tools as purely extending intelligence feels too constraining, could just as importantly be understood as imagination amplification; we‚Äôre seeing a lot of that too with generative IA.",karpathy,Andrej Karpathy,687,2,29,44
https://twitter.com/karpathy/status/1744179910347039080#m,"e/ia - Intelligence Amplification - Does not seek to build superintelligent God entity that replaces humans. - Builds ‚Äúbicycle for the mind‚Äù tools that empower and extend the information processing capabilities of humans. - Of all humans, not a top percentile. - Faithful to‚Ä¶",karpathy,Andrej Karpathy,5668,222,804,374
https://twitter.com/karpathy/status/1744063550749106484#m,"This is an existing but a bit dormant acronym, I didn‚Äôt make it up :)  https://en.m.wikipedia.org/wiki/Intelligence_amplification",karpathy,Andrej Karpathy,436,4,27,16
https://twitter.com/karpathy/status/1744062845426532473#m,"I‚Äôm playing around with calling our tech, as it is today, IA (intelligence amplification) instead of AI. IA have the vibe of tools for thought, needing human interaction, and resemble a lot more what we actually have today. AI feels more like independent long-running agents.",karpathy,Andrej Karpathy,3738,105,414,238
https://twitter.com/karpathy/status/1742320240938406133#m,"Shoutout to YouTube for solving the ""comments section"" problem of Computer Science. I recall at one point they used to be 90%+ toxic/spam, but in most videos I come by today the comments are almost surprisingly wholesome and informative.",karpathy,Andrej Karpathy,4781,19,179,251
https://twitter.com/karpathy/status/1742283766238957663#m,"""After 34 Years, Someone Finally Beat Tetris"" Wow, incredible video on what it took to beat Tetris, waaay beyond the game's original design.  Also a great reference for reinforcement learning and what superintelligence might look like.  https://tube.habedieeh.re/watch?v=GuJ5UuknsHU",karpathy,Andrej Karpathy,3154,69,419,108
https://twitter.com/karpathy/status/1740137276833943974#m,"""Operation Triangulation"" https://securelist.com/operation-triangulation-the-last-hardware-mystery/111669/  A newly discovered spyware campaign targeting Apple iPhone using a zero-click remote code execution via an attack chain of 4 zero-days, including highly mysterious, completely undocumented MMIO registers and hardware features‚Ä¶",karpathy,Andrej Karpathy,3947,135,778,143
https://twitter.com/karpathy/status/1740097030729683381#m,"The most unknown most common shortcut I use on my MacBook is:  - Command+Option+Shift+4 to select a small part of the screen and copy it into clipboard as an image - Command+Shift+4 to do the same, but save it as a file on Desktop as png  Life-changing.",karpathy,Andrej Karpathy,4874,74,267,571
https://twitter.com/karpathy/status/1740089842640531592#m,"I realized after posting that multi-tweet longform is user-hostile currently so I decided to convert and host it as a stand-alone markdown on my website too: https://karpathy.ai/blog/licklider1960.html  The ""conversion"" was a manual and work-intensive process. I wish that making simple markdown‚Ä¶",karpathy,Andrej Karpathy,141,0,11,22
https://twitter.com/karpathy/status/1740078753144037826#m,"The fun part of this, of course, is sliding the window, making the assumption of translation invariance in time. Imagine your own extrapolation of the future. And imagine its hindsight. Exercise left to the reader :)",karpathy,Andrej Karpathy,126,1,6,6
https://twitter.com/karpathy/status/1740078751223083277#m,"What would be the ""benefit of hindsight"" truths to tell Licklider at this time, with our knowledge today?  1. You're on the right track w.r.t. Intelligence Augmentation lasting a long time. And ""thinking centers"". 2. All of ""AI"" for *thinking* that you know and is currently‚Ä¶",karpathy,Andrej Karpathy,120,4,11,7
https://twitter.com/karpathy/status/1740078748513579445#m,"In the I/O section, Licklider also muses about adapting computers to human interfaces, in this case automatic speech recognition. Here, Licklider is significantly over-optimistic on capabilities, estimating 5 years to get it working. Here we are !!! 64 YEARS !!! later, and while‚Ä¶",karpathy,Andrej Karpathy,80,2,3,4
https://twitter.com/karpathy/status/1740078746500247770#m,"Licklider talks again and again about military applications of computing, I suppose that was top of mind in that era. I feel like this is, again, a misprediction about how computing would be used in society. Maybe it was talked about this way in some part because Licklider worked‚Ä¶",karpathy,Andrej Karpathy,59,0,2,4
